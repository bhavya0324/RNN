{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemanths03/CS6910_Assignment_3/blob/main/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z37HJ5CU5bM7"
      },
      "source": [
        "#Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m59A7f2p5bVn"
      },
      "outputs": [],
      "source": [
        "#Import the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import sample\n",
        "import random\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython.display import display\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "385sAN9E5i5N"
      },
      "source": [
        "#WandB Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "bQzB1BTz5f5h",
        "outputId": "df957c68-29aa-455d-9c00-846e16184c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 12.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 53.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 47.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmBGN7t15meQ"
      },
      "source": [
        "#Mounting the google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQnf1Q_z5q0v",
        "outputId": "0189c4d2-cb03-471d-e897-36394e3c0adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyt14pvD6DP9"
      },
      "source": [
        "#Load the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDRIrX6y6DbR"
      },
      "outputs": [],
      "source": [
        "train_dataset = pd.read_csv('/content/gdrive/MyDrive/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv', sep = '\\t', header = None)\n",
        "val_dataset = pd.read_csv('/content/gdrive/MyDrive/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv', sep = '\\t', header = None)\n",
        "test_dataset = pd.read_csv('/content/gdrive/MyDrive/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv', sep = '\\t', header = None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i6X7Rct5rCQ"
      },
      "source": [
        "#Preprocessing the dataset into required format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the datasets in input_lexicons and target_lexicons\n",
        "def split_dataset(dataset):\n",
        "  \n",
        "  temp = []\n",
        "  temp_ = []\n",
        "\n",
        "  for i in range(len(dataset)):\n",
        "    temp.append(str(dataset[1][i]))\n",
        "    temp_.append(\"\\t\"+str(dataset[0][i])+\"\\n\")\n",
        "\n",
        "  return temp , temp_"
      ],
      "metadata": {
        "id": "RPNHv3MZZnFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating unique character set and finding the max length word\n",
        "def unique_chars(words):\n",
        "\n",
        "  temp = set()\n",
        "  temp.add(' ')\n",
        "  \n",
        "  for word in words:\n",
        "    for char in word:\n",
        "      temp.add(char)\n",
        "\n",
        "  temp = sorted(list(temp))\n",
        "  var = max([len(word) for word in words])\n",
        "\n",
        "  return temp , var"
      ],
      "metadata": {
        "id": "NppTmVF7ZnOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating a token dictionary\n",
        "def token_dict(chars):\n",
        "\n",
        "  temp = dict([(char, i) for i, char in enumerate(chars)])\n",
        "\n",
        "  return temp"
      ],
      "metadata": {
        "id": "6vyyoIE0ZnRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate inverse token dictionary\n",
        "def reverse_dict(index):\n",
        "\n",
        "  temp = dict((i, char) for char, i in index.items())\n",
        "\n",
        "  return temp"
      ],
      "metadata": {
        "id": "Mqh99xeLZnTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate zeroes matrix\n",
        "def gen_zero_mat(lexicons , var):\n",
        "\n",
        "  temp = np.zeros((len(lexicons), var), dtype=\"float32\")\n",
        "\n",
        "  return temp"
      ],
      "metadata": {
        "id": "rpxkEZ8ZZuOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate zeroes tensor\n",
        "def gen_zero_ten(lexicons , var , var_):\n",
        "  \n",
        "  temp = np.zeros((len(lexicons), var, var_ ), dtype=\"float32\")\n",
        "\n",
        "  return temp"
      ],
      "metadata": {
        "id": "YHQYKoydZuQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Generation \n",
        "#Target Data is decoded is ahead of input data by one timestep\n",
        "#Target Data will be ahead by one timestep and will not include the start character.\n",
        "def datagen(input_lexicons , target_lexicons , encoder_input_data , input_index , decoder_input_data , target_index , decoder_target_data):\n",
        "  for i, (ip_text , tr_text) in enumerate(zip(input_lexicons , target_lexicons)):\n",
        "    for t, char in enumerate(ip_text):\n",
        "      encoder_input_data[i,t] = input_index[char]\n",
        "    encoder_input_data[i, t + 1 :] = input_index[' ']\n",
        "    for t, char in enumerate(tr_text):\n",
        "      decoder_input_data[i, t] = target_index[char]\n",
        "      if t > 0:\n",
        "        decoder_target_data[i , t - 1, target_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :] = target_index[' ']\n",
        "    decoder_target_data[i, t:,target_index[' ']] = 1.0\n",
        "\n",
        "  return encoder_input_data , decoder_input_data , decoder_target_data"
      ],
      "metadata": {
        "id": "FtKYlfaNZ3ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDYTVYFU5_us"
      },
      "outputs": [],
      "source": [
        "def preprocess_data():\n",
        "\n",
        "    #Splitting the datasets in input_lexicons and target_lexicons\n",
        "    train_input_lexicons , train_target_lexicons = split_dataset(train_dataset)\n",
        "\n",
        "    val_input_lexicons , val_target_lexicons = split_dataset(val_dataset)\n",
        "\n",
        "    test_input_lexicons , test_target_lexicons = split_dataset(test_dataset)\n",
        "\n",
        "    #union of all input words\n",
        "    ip_words = train_input_lexicons + val_input_lexicons + test_input_lexicons\n",
        "\n",
        "    #Union of all target words\n",
        "    op_words = train_target_lexicons + val_target_lexicons + test_target_lexicons\n",
        "\n",
        "\n",
        "    #Creating unique input and target character sets and Find the max sequence encoder and decoder length\n",
        "    input_characters , max_encoder_seq_length = unique_chars(ip_words)\n",
        "\n",
        "    target_characters , max_decoder_seq_length = unique_chars(op_words)\n",
        "\n",
        "    #Find the length of input and target characters\n",
        "    num_encoder_tokens = len(input_characters)\n",
        "\n",
        "    num_decoder_tokens = len(target_characters)\n",
        "\n",
        "    #Printing the dataset summary :\n",
        "    print(\"Summary of the dataset :\")\n",
        "    print(\"Number of train samples :\" , len(train_input_lexicons))\n",
        "    print(\"Number of val samples :\" , len(val_input_lexicons))\n",
        "    print(\"Number of test samples :\" , len(test_input_lexicons))\n",
        "    print(\"Number of unique input tokens :\" , num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens :\" , num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\" , max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\" , max_decoder_seq_length)\n",
        "\n",
        "    #Creating a dictionary for input words and target words\n",
        "    input_token_index = token_dict(input_characters)\n",
        "\n",
        "    target_token_index = token_dict(target_characters)\n",
        "\n",
        "    #Generate zeroes matrices\n",
        "    encoder_train_input_data = gen_zero_mat(train_input_lexicons , max_encoder_seq_length)\n",
        "\n",
        "    encoder_val_input_data = gen_zero_mat(val_input_lexicons , max_encoder_seq_length)\n",
        "\n",
        "    encoder_test_input_data = gen_zero_mat(test_input_lexicons, max_encoder_seq_length)\n",
        "\n",
        "    decoder_train_input_data = gen_zero_mat(train_input_lexicons , max_decoder_seq_length)\n",
        "\n",
        "    decoder_val_input_data = gen_zero_mat(val_input_lexicons , max_decoder_seq_length)\n",
        "\n",
        "    #Generate zeroes tensors\n",
        "    decoder_train_target_data = gen_zero_ten(train_input_lexicons , max_decoder_seq_length, num_decoder_tokens )\n",
        "    \n",
        "    decoder_val_target_data = gen_zero_ten(val_input_lexicons , max_decoder_seq_length, num_decoder_tokens)\n",
        "\n",
        "    \n",
        "    #TRAIN DATA\n",
        "    encoder_train_input_data , decoder_train_input_data , decoder_train_target_data = datagen(train_input_lexicons , train_target_lexicons , encoder_train_input_data , input_token_index , decoder_train_input_data , target_token_index , decoder_train_target_data)\n",
        "\n",
        "    #VALIDATION DATA\n",
        "    encoder_val_input_data , decoder_val_input_data , decoder_val_target_data = datagen(val_input_lexicons , val_target_lexicons , encoder_val_input_data , input_token_index , decoder_val_input_data , target_token_index , decoder_val_target_data)\n",
        "\n",
        "    #TEST DATA\n",
        "    for i, input_text in enumerate(test_input_lexicons):\n",
        "      for t, char in enumerate(input_text):\n",
        "          encoder_test_input_data[i, t] = input_token_index[char]\n",
        "      encoder_test_input_data[i, t + 1 :] = input_token_index[' ']\n",
        "      \n",
        "\n",
        "    return ((encoder_train_input_data , encoder_val_input_data , encoder_test_input_data),\n",
        "    (decoder_train_input_data , decoder_val_input_data),\n",
        "    (decoder_train_target_data , decoder_val_target_data),\n",
        "    (val_input_lexicons , test_input_lexicons),\n",
        "    (val_target_lexicons , test_target_lexicons),\n",
        "    (num_encoder_tokens , num_decoder_tokens),\n",
        "    (max_encoder_seq_length , max_decoder_seq_length),\n",
        "    (target_token_index , reverse_dict(input_token_index) , reverse_dict(target_token_index)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddNYxiS8ZSlz"
      },
      "source": [
        "#Load the preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qubNseHZWGm",
        "outputId": "df91b3e1-f723-4738-a141-9bf56c2ae412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of the dataset :\n",
            "Number of train samples : 58550\n",
            "Number of val samples : 5683\n",
            "Number of test samples : 5747\n",
            "Number of unique input tokens : 27\n",
            "Number of unique output tokens : 66\n",
            "Max sequence length for inputs: 25\n",
            "Max sequence length for outputs: 22\n"
          ]
        }
      ],
      "source": [
        "((encoder_train_input_data , encoder_val_input_data , encoder_test_input_data),\n",
        " (decoder_train_input_data , decoder_val_input_data),\n",
        " (decoder_train_target_data , decoder_val_target_data),\n",
        " (val_input_lexicons , test_input_lexicons),\n",
        " (val_target_lexicons , test_target_lexicons),\n",
        " (num_encoder_tokens , num_decoder_tokens),\n",
        " (max_encoder_seq_length , max_decoder_seq_length),\n",
        " (target_token_index , inverse_input_token_index , inverse_target_token_index)) = preprocess_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Some functions for Visualization Purpose"
      ],
      "metadata": {
        "id": "2xVbZA0STYgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the color font\n",
        "def cstr(s, color = 'black'):\n",
        "    return \"<text style=color:#000;padding-top:1.5px;padding-bottom:1.5px;padding-left:2.5px;padding-right:2.5px;background-color:{}>{} </text>\".format(color, s)"
      ],
      "metadata": {
        "id": "ZECtGpKdSFdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get color function\n",
        "def get_clr(value, mode):\n",
        "    if(mode == 'l'):\n",
        "        colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8', '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8', '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f', '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
        "        value = int((value * 100) / 5)\n",
        "        return colors[value]\n",
        "    else:\n",
        "        colors = ['#ffffff', '#ecf7fb', '#daeff7', '#c7e7f3', '#b5dfef', '#a2d7eb', '#90cfe7', '#7dc7e3', '#6abfdf', '#58b7db', '#46afd7']\n",
        "        value = int((value * 100) / 10)\n",
        "        return colors[value]"
      ],
      "metadata": {
        "id": "xBhiz50iSFvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_c(dec_char, text_colours):\n",
        "    if (dec_char == \"<e>\"):\n",
        "      display(HTML(''.join([cstr(ti, color = ci) for ti, ci in text_colours]) + \" <b> &emsp; &lt; e &gt; </b>  &emsp; &nbsp; \"))\n",
        "    else:\n",
        "      display(HTML(''.join([cstr(ti, color = ci) for ti, ci in text_colours]) + \" <b> &emsp; {}</b>  &emsp; &emsp; \".format(dec_char)))"
      ],
      "metadata": {
        "id": "zDO16qMmSGAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_l(dec_seq, prob):\n",
        "    text_colours = []\n",
        "\n",
        "    for c, p in zip(dec_seq, prob): \n",
        "        text = (c, get_clr(p, 'l'))\n",
        "        text_colours.append(text)\n",
        "    \n",
        "    display(HTML(''.join([cstr(ti, color = ci) for ti, ci in text_colours])))"
      ],
      "metadata": {
        "id": "kpIYzTzgSGSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Connectivity Visualization function"
      ],
      "metadata": {
        "id": "gxtvx8YAQWP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_connectivity(N):\n",
        "\n",
        "    # Reading from conv_vis file\n",
        "    with open(\"visualize.txt\", \"r\", encoding='utf-8') as filepointer:\n",
        "        \n",
        "        lines = filepointer.readlines()\n",
        "\n",
        "        i = 0\n",
        "        words_visualized = 0\n",
        "\n",
        "        while i < len(lines) and  words_visualized< N:\n",
        "            line = lines[i]\n",
        "            \n",
        "            if line[:4] == \"Next\":\n",
        "                words_visualized += 1\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if line[:4] != \"Next\": \n",
        "                true_word, dec_char_len = line.split('\\t') \n",
        "                dec_word_len = int(dec_char_len)\n",
        "                i += 1\n",
        "\n",
        "                true_word_array = [c for c in true_word]\n",
        "\n",
        "                for j in range(dec_word_len):\n",
        "                    line = lines[i]\n",
        "                    line = line.split('\\t')\n",
        "  \n",
        "                    dec_char = line[0]\n",
        "                    text_colours = []\n",
        "\n",
        "                    prob = []\n",
        "                    for prob_index in range(1,len(true_word)+1) :\n",
        "                        p = float(line[prob_index])\n",
        "                        prob.append(p)\n",
        "\n",
        "                    line = softmax(prob)\n",
        "\n",
        "                    \n",
        "                    for prob_index in range(len(true_word)) :\n",
        "                        p = float(line[prob_index])\n",
        "\n",
        "                        true_char = true_word_array[prob_index]\n",
        "                        text= (true_char, get_clr(p, 'c') )\n",
        "                        text_colours.append(text)\n",
        "\n",
        "                    visualize_c(dec_char, text_colours)\n",
        "            \n",
        "                    i += 1\n",
        "\n",
        "            print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "Jbt6IrohQWY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM Visualization function"
      ],
      "metadata": {
        "id": "ziBtGchdS2pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_lstm(N, neuron):\n",
        "\n",
        "    for i in range(N):\n",
        "\n",
        "        file = open(\"lstm_viz_\" + str(i) + \".txt\", \"r\")\n",
        "        input_seq = file.readline()[:-1]\n",
        "        \n",
        "        dec_seq = []\n",
        "        prob = []\n",
        "\n",
        "        for line in file:\n",
        "            temp = line.split('\\t')\n",
        "            dec_seq.append(temp[0])\n",
        "            prob.append(ast.literal_eval(temp[1][:-1])[neuron - 1])\n",
        "\n",
        "        visualize_l(dec_seq, prob)\n",
        "        print()"
      ],
      "metadata": {
        "id": "oPTM3zLCS5Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to plot the heatmaps"
      ],
      "metadata": {
        "id": "QC5B2yiBLmfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_heatmap(input_word, heatmap_data):\n",
        "    mats = []\n",
        "    dec_inputs = []\n",
        "\n",
        "    for data in heatmap_data:\n",
        "        dec_ind, attn  = data[0], data[1]\n",
        "        mats.append(attn.reshape(-1)[:len(input_word)])\n",
        "        dec_inputs.append(dec_ind)\n",
        "    \n",
        "    attention_mat = np.array(mats)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(attention_mat)\n",
        "\n",
        "    ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
        "    ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
        "\n",
        "    ax.set_yticklabels([inp if inp != '\\n' else \"<e>\" for inp in dec_inputs], fontproperties = FontProperties(fname = \"/content/gdrive/MyDrive/Fonts/nirmala.ttf\"))\n",
        "    ax.set_xticklabels([char for char in input_word])\n",
        "\n",
        "    ax.tick_params(labelsize = 15)\n",
        "    ax.tick_params(axis = 'x', labelrotation =  45)\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "zKA93tmbLmpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sigmoid function"
      ],
      "metadata": {
        "id": "Lwv5tLYNQ6wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sigmoid function\n",
        "def sigmoid(x):\n",
        "    temp = []\n",
        "    for i in range(len(x)):\n",
        "      temp.append(1/(1 + np.exp(-x[i])))\n",
        "    return list(temp)"
      ],
      "metadata": {
        "id": "uCnes5yvQ8GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Softmax Function"
      ],
      "metadata": {
        "id": "af_d5GZDQwD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Softmax function\n",
        "def softmax(x):\n",
        "    denom = sum([np.exp(i) for i in x])\n",
        "    return [np.exp(i) / denom for i in x]"
      ],
      "metadata": {
        "id": "RvI2MpjDQpv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building an Attention layer"
      ],
      "metadata": {
        "id": "O2nOR5iAAqm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "class AttentionLayer(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape = tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer = 'uniform',\n",
        "                                   trainable = True)\n",
        "\n",
        "        self.U_a = self.add_weight(name = 'U_a',\n",
        "                                   shape = tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer = 'uniform',\n",
        "                                   trainable = True)\n",
        "\n",
        "        self.V_a = self.add_weight(name = 'V_a',\n",
        "                                   shape = tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer = 'uniform',\n",
        "                                   trainable = True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs):\n",
        "       \n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "           \n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "            \n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs"
      ],
      "metadata": {
        "id": "EdBro67o_-4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decode sequence"
      ],
      "metadata": {
        "id": "lTO2hOyCR44E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq , encoder_model , decoder_model , target_token_index , inverse_target_token_index , max_decoder_seq_length):\n",
        "\n",
        "    #Declare heatmapss data and viz_data\n",
        "    heatmap_data = []\n",
        "    viz_data = []\n",
        "\n",
        "    #Encode the input as state vectors.\n",
        "    encoder_outputs = encoder_model.predict(input_seq)\n",
        "    encoder_output , states_value = encoder_outputs[0] , encoder_outputs[1:]\n",
        "\n",
        "    #Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    #Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "        \n",
        "    #Sampling loop for a batch of sequences\n",
        "    flag = False\n",
        "    decoded_sentence = \"\"  \n",
        "\n",
        "    while not flag:\n",
        "\n",
        "          output = decoder_model.predict([target_seq] + states_value + [encoder_output])\n",
        "          output_tokens, states_value, attention_weights = output[0], output[1:-1], output[-1]\n",
        "\n",
        "          #Sampling a token\n",
        "          sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "          sampled_char = inverse_target_token_index[sampled_token_index]\n",
        "          decoded_sentence += sampled_char\n",
        "\n",
        "          if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "              flag = True\n",
        "\n",
        "          target_seq = np.zeros((1, 1))\n",
        "          target_seq[0, 0] = sampled_token_index\n",
        "          heatmap_data.append((sampled_char,attention_weights))\n",
        "          viz_data.append((sampled_char,states_value[0]))\n",
        "\n",
        "    return decoded_sentence , heatmap_data , viz_data"
      ],
      "metadata": {
        "id": "nxU2Pn2gR5BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference call to calculate the accuracy\n",
        "1.)encode input and retrieve initial decoder state\n",
        "\n",
        "2.)run one step of decoder with this initial state and a \"start of sequence\" token as target. Output will be the next target token.\n",
        "\n",
        "3.)Repeat with the current target token and current states"
      ],
      "metadata": {
        "id": "IsHtISe9Nemv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(encoder_test_input_data, test_input_lexicons, test_target_lexicons, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, latent_dim ,cell_type ,samples):\n",
        "      # Define sampling models\n",
        "      # Restore the model and construct the encoder and decoder.\n",
        "      model = keras.models.load_model(\"seq2seq_2\")\n",
        "\n",
        "      #Declare index variable for encoder embedding and decoder embedding \n",
        "\n",
        "      count = 0\n",
        "      vc = 0\n",
        "      size_ = len(test_input_lexicons)\n",
        "      maps = []\n",
        "      viz_inputs = sample(range(size_),samples)\n",
        "\n",
        "      #Creating a .csv for to write predictions\n",
        "      predictions_attention_RNN = open(\"predictions_attention_RNN.csv\", \"w\", encoding='utf-8')\n",
        "      predictions_attention_RNN.write(\"Input Word,Predicted Word,True Word\\n\")\n",
        "\n",
        "      #Defining the encoder\n",
        "      encoder_inputs = model.input[0] #input_1\n",
        "\n",
        "      if cell_type == \"rnn\" or cell_type == \"gru\" :\n",
        "        encoder_outputs , state = model.layers[4].output\n",
        "        encoder_model = keras.Model(encoder_inputs , [encoder_outputs] + [state])\n",
        "      elif cell_type == \"lstm\" :\n",
        "        encoder_outputs , state_h_enc , state_c_enc = model.layers[4].output\n",
        "        encoder_model = keras.Model(encoder_inputs , [encoder_outputs] + [state_h_enc , state_c_enc])\n",
        "      else :\n",
        "         print(\"Write the Required cell type encoder definition here...!\")\n",
        "         return\n",
        "\n",
        "      #Defining the Decoder\n",
        "      decoder_inputs = model.input[1]  # input_2\n",
        "      decoder_outputs =  model.layers[3](decoder_inputs)\n",
        "\n",
        "      '''decoder_states_inputs =  []\n",
        "      decoder_states = []'''\n",
        "\n",
        "      #Decoder\n",
        "      if cell_type == \"rnn\" or cell_type == \"gru\":\n",
        "          state = keras.Input(shape = (latent_dim, ))\n",
        "          current_states_inputs = [state]\n",
        "          decoder_outputs, state = model.layers[5](decoder_outputs, initial_state = current_states_inputs)\n",
        "          decoder_states = [state]\n",
        "\n",
        "      elif cell_type == \"lstm\":\n",
        "          state_h_dec, state_c_dec = keras.Input(shape = (latent_dim,)),  keras.Input(shape = (latent_dim,))\n",
        "          current_states_inputs = [state_h_dec, state_c_dec]\n",
        "          decoder_outputs, state_h_dec,state_c_dec = model.layers[5](decoder_outputs, initial_state = current_states_inputs)\n",
        "          decoder_states = [state_h_dec, state_c_dec]\n",
        "\n",
        "\n",
        "      #Attention\n",
        "      attention_inputs = keras.Input(shape = (None, latent_dim, ))\n",
        "      attention_output, attention_scores = model.layers[6]([attention_inputs, decoder_outputs])\n",
        "      decoder_input_concate = model.layers[7]([decoder_outputs, attention_output])\n",
        "\n",
        "\n",
        "      # Dense layer\n",
        "      decoder_outputs = model.layers[8](decoder_input_concate)\n",
        "\n",
        "      # Decoder model\n",
        "      decoder_model = keras.Model([decoder_inputs] + current_states_inputs + [attention_inputs] , [decoder_outputs] + decoder_states + [attention_scores])\n",
        "\n",
        "\n",
        "      for word_ind in range(size_): \n",
        "       \n",
        "          input_seq = encoder_test_input_data[word_ind : word_ind + 1]\n",
        "\n",
        "          decoded_word , heatmap_data , viz_data = decode_sequence(input_seq , encoder_model , decoder_model , target_token_index , inverse_target_token_index , max_decoder_seq_length)\n",
        "\n",
        "          orig_word = test_target_lexicons[word_ind][1:]\n",
        "\n",
        "          predictions_attention_RNN.write(test_input_lexicons[word_ind] + \",\" + decoded_word[:-1] + \",\" + orig_word[:-1] + \"\\n\")\n",
        "          \n",
        "          if(orig_word == decoded_word): count += 1\n",
        "\n",
        "          if word_ind in viz_inputs:\n",
        "            #Plot the heatmap\n",
        "            hmap = plot_heatmap(test_input_lexicons[word_ind] , heatmap_data)\n",
        "            maps.append(hmap)\n",
        "\n",
        "            #Visualize the connectivity\n",
        "            with open(\"visualize.txt\" , \"a\" , encoding=\"utf-8\") as filepointer:\n",
        "              #Compute heatmap and true word\n",
        "              true_word = test_input_lexicons[word_ind]\n",
        "\n",
        "              #Writing data into the conv_vis.txt file for visualisation purpose\n",
        "              filepointer.write(true_word)\n",
        "              filepointer.write(\"\\t\")\n",
        "              filepointer.write(str(len(heatmap_data)))\n",
        "              filepointer.write(\"\\n\")\n",
        "\n",
        "              for tup in range(len(heatmap_data)):\n",
        "                  dec_char = heatmap_data[tup][0]\n",
        "                  dec_char_prob = heatmap_data[tup][1].reshape(-1)\n",
        "                \n",
        "                  if tup == len(heatmap_data) - 1:\n",
        "                      filepointer.write(\"<e>\")\n",
        "                  else:\n",
        "                      filepointer.write(dec_char)\n",
        "                    \n",
        "                  filepointer.write(\"\\t\")\n",
        "\n",
        "                  for p in range(len(true_word)):\n",
        "                      filepointer.write(str(dec_char_prob[p]))\n",
        "                      filepointer.write(\"\\t\")\n",
        "\n",
        "                  filepointer.write(\"\\n\")\n",
        "\n",
        "              filepointer.write(\"Next\\n\")\n",
        "\n",
        "            #LSTM VISUALIZATION\n",
        "            file = open(\"lstm_viz_\" + str(vc) + \".txt\", \"w\", encoding='utf-8')\n",
        "            file.write(test_input_lexicons[word_ind] + \"\\n\")\n",
        "\n",
        "            for i, data in enumerate(viz_data):\n",
        "              dec_char, neuron_activation  = data[0], sigmoid(data[1].reshape(-1))\n",
        "              if i == len(viz_data) - 1:\n",
        "                  file.write(\"<e>\" + \"\\t\" + str(neuron_activation) + \"\\n\")\n",
        "              else:\n",
        "                  file.write(dec_char + \"\\t\" + str(neuron_activation) + \"\\n\")\n",
        "\n",
        "            vc += 1\n",
        "\n",
        "      return count / size_ , maps"
      ],
      "metadata": {
        "id": "F6TD9suxNfi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJVhJdbBf4WU"
      },
      "source": [
        "#Building an RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aLBHVDNdtwl"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "\n",
        "    np.random.seed(77)\n",
        "\n",
        "    #Initializing WandB\n",
        "    run = wandb.init()\n",
        "    config = run.config\n",
        "\n",
        "    #Setting up the Run name\n",
        "    name = \"ES_\" + str(config.embedding_size) + \"_CT_\" + config.cell_type + \"_DO_\" + str(config.dropout) + \"_BS_\" + str(config.beam_size)\n",
        "    run.name = name\n",
        "\n",
        "    #Number of samples to visualize\n",
        "    samples = 10\n",
        "\n",
        "    #Define input sequence and Setting the Encoder\n",
        "    encoder_inputs = keras.Input(shape =(None, ))\n",
        "    encoder_outputs = keras.layers.Embedding(input_dim = num_encoder_tokens, output_dim = config.embedding_size, input_length = max_encoder_seq_length)(encoder_inputs)\n",
        "\n",
        "    #Setting the Decoder\n",
        "    decoder_inputs = keras.Input(shape=(None, ))\n",
        "    decoder_outputs = keras.layers.Embedding(input_dim = num_decoder_tokens, output_dim = config.embedding_size, input_length = max_decoder_seq_length)(decoder_inputs)\n",
        "\n",
        "    # We discard encoder_outputs and only keep the states.\n",
        "    # Set up the decoder, using encoder_states as initial state.\n",
        "    encoder_states = list()\n",
        "\n",
        "    if config.cell_type == \"rnn\":\n",
        "        encoder_outputs, state = keras.layers.SimpleRNN(config.hidden_layer_size, dropout = config.dropout, return_state = True, recurrent_dropout = config.recurrent_dropout, return_sequences = True)(encoder_outputs)\n",
        "        encoder_states = [state]\n",
        "    if config.cell_type == \"lstm\":\n",
        "        encoder_outputs, state_h, state_c = keras.layers.LSTM(config.hidden_layer_size, dropout = config.dropout, return_state = True, recurrent_dropout = config.recurrent_dropout, return_sequences = True)(encoder_outputs)\n",
        "        encoder_states = [state_h,state_c]\n",
        "    if config.cell_type == \"gru\":\n",
        "        encoder_outputs, state = keras.layers.GRU(config.hidden_layer_size, dropout = config.dropout, return_state = True, recurrent_dropout = config.recurrent_dropout, return_sequences = True)(encoder_outputs)\n",
        "        encoder_states = [state]\n",
        "\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the\n",
        "    # return states in the training model, but we will use them in inference.\n",
        "\n",
        "    if config.cell_type == \"rnn\":\n",
        "        decoder = keras.layers.SimpleRNN(config.hidden_layer_size, dropout = config.dropout, return_sequences = True, recurrent_dropout = config.recurrent_dropout, return_state = True)\n",
        "        decoder_outputs, state = decoder(decoder_outputs, initial_state = encoder_states)\n",
        "        decoder_states = [state]\n",
        "    if config.cell_type == \"lstm\":\n",
        "        decoder = keras.layers.LSTM(config.hidden_layer_size, dropout = config.dropout, return_sequences = True, recurrent_dropout = config.recurrent_dropout, return_state = True)\n",
        "        decoder_outputs, state_h, state_c = decoder(decoder_outputs, initial_state = encoder_states)\n",
        "        decoder_states = [state_h , state_c]\n",
        "    if config.cell_type == \"gru\":\n",
        "        decoder = keras.layers.GRU(config.hidden_layer_size, dropout = config.dropout, return_sequences = True, recurrent_dropout = config.recurrent_dropout, return_state = True)\n",
        "        decoder_outputs, state = decoder(decoder_outputs, initial_state = encoder_states)\n",
        "        decoder_states = [state]\n",
        "    \n",
        "\n",
        "    # Attention\n",
        "    attention_mech = AttentionLayer()\n",
        "    attention_output, _ = attention_mech([encoder_outputs, decoder_outputs])\n",
        "    decoder_input_concate = keras.layers.Concatenate(axis = -1)([decoder_outputs, attention_output])\n",
        "\n",
        "    #Adding a dense layer\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation = \"softmax\")\n",
        "    decoder_outputs = decoder_dense(decoder_input_concate)\n",
        "\n",
        "    # Define the model that will turn on\n",
        "    # encoder_train_input_data & decoder_train_input_data into decoder_train_target_data\n",
        "\n",
        "    model = keras.Model([encoder_inputs , decoder_inputs] , decoder_outputs)\n",
        "\n",
        "    #Compiling the model\n",
        "    model.compile(optimizer=config.learning_algo , loss = \"categorical_crossentropy\" , metrics=[\"accuracy\"])\n",
        "\n",
        "    #Fitting the model\n",
        "    model.fit(\n",
        "        [encoder_train_input_data, decoder_train_input_data],\n",
        "        decoder_train_target_data,\n",
        "        batch_size = config.batch_size,\n",
        "        epochs = config.epochs,\n",
        "        callbacks = [WandbCallback()]\n",
        "    )\n",
        "\n",
        "    #save the model\n",
        "    model.save(\"seq2seq_2\")\n",
        "\n",
        "    #Calculating validation accuracy using inference on validation data\n",
        "    val_accuracy , maps = run_inference(encoder_val_input_data, val_input_lexicons, val_target_lexicons, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, config.hidden_layer_size ,config.cell_type,samples)\n",
        "    print(\"VALIDATION ACCURACY :\" , val_accuracy)\n",
        "    wandb.log({\"val_accuracy\": val_accuracy})\n",
        "\n",
        "    #Calculating test accuracy using inference on test data\n",
        "    test_accuracy , maps = run_inference(encoder_test_input_data, test_input_lexicons, test_target_lexicons, num_decoder_tokens, max_decoder_seq_length, target_token_index, inverse_target_token_index, config.hidden_layer_size , config.cell_type,samples)\n",
        "    print(\"TEST ACCURACY :\" , test_accuracy)\n",
        "    wandb.log({\"test_accuracy\": test_accuracy})\n",
        "\n",
        "    #Logging the generated heatmaps into wandb\n",
        "    for i,heatmap in enumerate(maps):\n",
        "      wandb.log({\"heat_map_\"+str(i):heatmap})\n",
        "\n",
        "    #Connectivity visualization\n",
        "    visualize_connectivity(samples)\n",
        "\n",
        "    #Lstm visualization for 3 random neurons\n",
        "    neuron_no = random.choice(range(config.hidden_layer_size))\n",
        "    print(\"Neuron :\",neuron_no)\n",
        "    visualize_lstm(samples, neuron_no)\n",
        "\n",
        "    neuron_no = random.choice(range(config.hidden_layer_size))\n",
        "    print(\"Neuron :\",neuron_no)\n",
        "    visualize_lstm(samples, neuron_no)\n",
        "\n",
        "    neuron_no = random.choice(range(config.hidden_layer_size))\n",
        "    print(\"Neuron :\",neuron_no)\n",
        "    visualize_lstm(samples, neuron_no)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oozOLsvkg1Jt"
      },
      "source": [
        "#Sweep Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Actual Sweep Configuration"
      ],
      "metadata": {
        "id": "DCOfCqCgap45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''sweep_config_temp = {\n",
        "  \"name\": \"Actual_sweep\",\n",
        "\n",
        "  \"method\": \"bayes\",\n",
        "\n",
        "  \"early_terminate\": {\n",
        "      \"type\": \"hyperband\",\n",
        "      \"min_iter\": 3\n",
        "    },\n",
        "\n",
        "  \"metric\": {\n",
        "      \"name\": \"accuracy\",\n",
        "      \"goal\": \"maximize\"  \n",
        "    },\n",
        "    \n",
        "  \"parameters\": {\n",
        "        \"batch_size\": {\n",
        "            \"values\": [128 , 256]\n",
        "        },\n",
        "        \"beam_size\": {\n",
        "          \"values\": [0.0]\n",
        "        },\n",
        "        \"cell_type\": {\n",
        "            \"values\": [\"gru\" , \"lstm\" , \"rnn\"]\n",
        "        },\n",
        "        \"decoder_layers\": {\n",
        "            \"values\": [1]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0.0 , 0.1 , 0.2 , 0.3]\n",
        "        },\n",
        "        \"embedding_size\": {\n",
        "            \"values\": [64 , 128 , 256]\n",
        "        },\n",
        "        \"encoder_layers\" :{\n",
        "            \"values\" : [1]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [25]\n",
        "        },\n",
        "        \"hidden_layer_size\": {\n",
        "            \"values\": [64 , 128 , 256]\n",
        "        },\n",
        "        \"learning_algo\": {\n",
        "            \"values\": [\"adam\" , \"rmsprop\"]\n",
        "        },\n",
        "        \"recurrent_dropout\": {\n",
        "            \"values\": [0.0 , 0.1]\n",
        "        }\n",
        "    }\n",
        "}'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Q_l6aZAoaqBp",
        "outputId": "f125d3d5-30cd-4011-e260-3cb7265ccf91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sweep_config_temp = {\\n  \"name\": \"Actual_sweep\",\\n\\n  \"method\": \"bayes\",\\n\\n  \"early_terminate\": {\\n      \"type\": \"hyperband\",\\n      \"min_iter\": 3\\n    },\\n\\n  \"metric\": {\\n      \"name\": \"accuracy\",\\n      \"goal\": \"maximize\"  \\n    },\\n    \\n  \"parameters\": {\\n        \"batch_size\": {\\n            \"values\": [128 , 256]\\n        },\\n        \"beam_size\": {\\n          \"values\": [0.0]\\n        },\\n        \"cell_type\": {\\n            \"values\": [\"gru\" , \"lstm\" , \"rnn\"]\\n        },\\n        \"decoder_layers\": {\\n            \"values\": [1]\\n        },\\n        \"dropout\": {\\n            \"values\": [0.0 , 0.1 , 0.2 , 0.3]\\n        },\\n        \"embedding_size\": {\\n            \"values\": [64 , 128 , 256]\\n        },\\n        \"encoder_layers\" :{\\n            \"values\" : [1]\\n        },\\n        \"epochs\": {\\n            \"values\": [25]\\n        },\\n        \"hidden_layer_size\": {\\n            \"values\": [64 , 128 , 256]\\n        },\\n        \"learning_algo\": {\\n            \"values\": [\"adam\" , \"rmsprop\"]\\n        },\\n        \"recurrent_dropout\": {\\n            \"values\": [0.0 , 0.1]\\n        }\\n    }\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Best Model Sweep Configuration"
      ],
      "metadata": {
        "id": "m_7vXI9HaqKe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUf1_wUdg1Tn"
      },
      "outputs": [],
      "source": [
        "sweep_config_temp = {\n",
        "  \"name\": \"Best_sweep\",\n",
        "\n",
        "  \"method\": \"bayes\",\n",
        "\n",
        "  \"early_terminate\": {\n",
        "      \"type\": \"hyperband\",\n",
        "      \"min_iter\": 3\n",
        "    },\n",
        "\n",
        "  \"metric\": {\n",
        "      \"name\": \"accuracy\",\n",
        "      \"goal\": \"maximize\"  \n",
        "    },\n",
        "    \n",
        "  \"parameters\": {\n",
        "        \"batch_size\": {\n",
        "            \"values\": [128]\n",
        "        },\n",
        "        \"beam_size\": {\n",
        "            \"values\": [0.0]\n",
        "        },\n",
        "        \"cell_type\": {\n",
        "            \"values\": [\"lstm\"]\n",
        "        },\n",
        "        \"decoder_layers\": {\n",
        "            \"values\": [1]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0.3]\n",
        "        },\n",
        "        \"embedding_size\": {\n",
        "            \"values\": [256]\n",
        "        },\n",
        "        \"encoder_layers\" :{\n",
        "            \"values\" : [1]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [25]\n",
        "        },\n",
        "        \"hidden_layer_size\": {\n",
        "            \"values\": [256]\n",
        "        },\n",
        "        \"learning_algo\": {\n",
        "            \"values\": [\"adam\"]\n",
        "        },\n",
        "        \"recurrent_dropout\": {\n",
        "            \"values\": [0.0]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config_temp, entity=\"cs21m027_cs21m011\", project=\"DL_ASG_3_final_Attention\")\n",
        "wandb.agent(sweep_id, train)"
      ],
      "metadata": {
        "id": "mAziqqufZUqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q74TCOhSbuAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LUfkvfs6Mclu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bsJnCinmbuCI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Attention.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}