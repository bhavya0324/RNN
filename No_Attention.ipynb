{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "No_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKzIDF5JEo1yFeX7lY7u6G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemanths03/CS6910_Assignment_3/blob/main/No_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Required Libraries"
      ],
      "metadata": {
        "id": "z37HJ5CU5bM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import sample\n",
        "\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "m59A7f2p5bVn"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WandB Login"
      ],
      "metadata": {
        "id": "385sAN9E5i5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "bQzB1BTz5f5h"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mounting the google drive"
      ],
      "metadata": {
        "id": "YmBGN7t15meQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQnf1Q_z5q0v",
        "outputId": "f0e92d79-dbc2-4df8-d7a9-3ad3dd625b80"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the datasets"
      ],
      "metadata": {
        "id": "tyt14pvD6DP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_csv('/content/gdrive/MyDrive/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv', sep = '\\t', header = None)\n",
        "val_dataset = pd.read_csv('/content/gdrive/MyDrive/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv', sep = '\\t', header = None)\n",
        "test_dataset = pd.read_csv('/content/gdrive/MyDrive/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv', sep = '\\t', header = None)"
      ],
      "metadata": {
        "id": "FDRIrX6y6DbR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing the dataset into required format"
      ],
      "metadata": {
        "id": "6i6X7Rct5rCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data():\n",
        "\n",
        "    #Declaring some required variables\n",
        "\n",
        "    train_input_lexicons = []\n",
        "    train_target_lexicons = []\n",
        "    val_input_lexicons = []\n",
        "    val_target_lexicons = []\n",
        "    test_input_lexicons = []\n",
        "    test_target_lexicons = []\n",
        "\n",
        "    #Splitting the datasets in input_lexicons and target_lexicons\n",
        "\n",
        "    for i in range(len(train_dataset)):\n",
        "      train_input_lexicons.append(str(train_dataset[1][i]))\n",
        "      train_target_lexicons.append(\"\\t\" + str(train_dataset[0][i])+\"\\n\")\n",
        "\n",
        "    for i in range(len(val_dataset)):\n",
        "      val_input_lexicons.append(str(val_dataset[1][i]))\n",
        "      val_target_lexicons.append(\"\\t\" + str(val_dataset[0][i])+\"\\n\")\n",
        "\n",
        "    for i in range(len(test_dataset)):\n",
        "      test_input_lexicons.append(str(test_dataset[1][i]))\n",
        "      test_target_lexicons.append(\"\\t\" + str(test_dataset[0][i])+\"\\n\")\n",
        "\n",
        "\n",
        "    #Creating unique input and target character sets\n",
        "\n",
        "    input_characters = set()\n",
        "    input_characters.add(' ')\n",
        "    target_characters = set()\n",
        "    target_characters.add(' ')\n",
        "\n",
        "    #union of all input words\n",
        "\n",
        "    ip_words = train_input_lexicons + val_input_lexicons + test_input_lexicons\n",
        "\n",
        "    #Union of all target words\n",
        "\n",
        "    op_words = train_target_lexicons + val_target_lexicons + test_target_lexicons\n",
        "\n",
        "    #Adding unique characters in their respective sets\n",
        "\n",
        "    for word in ip_words:\n",
        "      for char in word:\n",
        "        input_characters.add(char)\n",
        "\n",
        "    for word in op_words:\n",
        "      for char in word:\n",
        "        target_characters.add(char)\n",
        "\n",
        "    #Sorting the list\n",
        "\n",
        "    input_characters = sorted(list(input_characters))\n",
        "    num_encoder_tokens = len(input_characters)\n",
        "\n",
        "    target_characters = sorted(list(target_characters))\n",
        "    num_decoder_tokens = len(target_characters)\n",
        "\n",
        "    #Find the max sequence length input and target\n",
        "\n",
        "    max_encoder_seq_length = max([len(word) for word in ip_words])\n",
        "    max_decoder_seq_length = max([len(word) for word in op_words])\n",
        "\n",
        "    #Printing the summary :\n",
        "\n",
        "    print(\"Summary of the dataset :\")\n",
        "    print(\"Number of train samples :\" , len(train_input_lexicons))\n",
        "    print(\"Number of val samples :\" , len(val_input_lexicons))\n",
        "    print(\"Number of test samples :\" , len(test_input_lexicons))\n",
        "    print(\"Number of unique input tokens :\" , num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens :\" , num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\" , max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\" , max_decoder_seq_length)\n",
        "\n",
        "    #Creating a dictionary for input words and target words\n",
        "\n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "\n",
        "    encoder_train_input_data = np.zeros((len(train_input_lexicons), max_encoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "    encoder_val_input_data = np.zeros((len(val_input_lexicons), max_encoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "    encoder_test_input_data = np.zeros((len(test_input_lexicons), max_encoder_seq_length), dtype=\"float32\")\n",
        "\n",
        "\n",
        "    decoder_train_input_data = np.zeros((len(train_input_lexicons), max_decoder_seq_length), dtype=\"float32\")\n",
        "    decoder_train_target_data = np.zeros((len(train_input_lexicons), max_decoder_seq_length, num_decoder_tokens ), dtype=\"float32\")\n",
        "\n",
        "   \n",
        "    decoder_val_input_data = np.zeros((len(val_input_lexicons), max_decoder_seq_length), dtype=\"float32\")\n",
        "    decoder_val_target_data = np.zeros((len(val_input_lexicons), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "    \n",
        "    #TRAIN DATA\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(train_input_lexicons, train_target_lexicons)):\n",
        "      for t, char in enumerate(input_text):\n",
        "          encoder_train_input_data[i, t] = input_token_index[char]\n",
        "      encoder_train_input_data[i, t + 1 :] = input_token_index[' ']\n",
        "      for t, char in enumerate(target_text):\n",
        "          # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "          decoder_train_input_data[i, t] = target_token_index[char]\n",
        "          if t > 0:\n",
        "              # decoder_target_data will be ahead by one timestep\n",
        "              # and will not include the start character.\n",
        "              decoder_train_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "      decoder_train_input_data[i, t + 1 :] = target_token_index[' ']\n",
        "      decoder_train_target_data[i, t:, target_token_index[' ']] = 1.0\n",
        "\n",
        "    #VALIDATION DATA\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(val_input_lexicons, val_target_lexicons)):\n",
        "      for t, char in enumerate(input_text):\n",
        "          encoder_val_input_data[i, t] = input_token_index[char]\n",
        "      encoder_val_input_data[i, t + 1 :] = input_token_index[' ']\n",
        "      for t, char in enumerate(target_text):\n",
        "          # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "          decoder_val_input_data[i, t] = target_token_index[char]\n",
        "          if t > 0:\n",
        "              # decoder_target_data will be ahead by one timestep\n",
        "              # and will not include the start character.\n",
        "              decoder_val_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "      decoder_val_input_data[i, t + 1 :] = target_token_index[' ']\n",
        "      decoder_val_target_data[i, t:, target_token_index[' ']] = 1.0\n",
        "\n",
        "    #TEST DATA\n",
        "\n",
        "    for i, input_text in enumerate(test_input_lexicons):\n",
        "      for t, char in enumerate(input_text):\n",
        "          encoder_test_input_data[i, t] = input_token_index[char]\n",
        "      encoder_test_input_data[i, t + 1 :] = input_token_index[' ']\n",
        "      \n",
        "\n",
        "    inverse_input_token_index = dict((i, char) for char, i in input_token_index.items())\n",
        "    inverse_target_token_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "    return ((encoder_train_input_data , encoder_val_input_data , encoder_test_input_data),\n",
        "    (decoder_train_input_data , decoder_val_input_data),\n",
        "    (decoder_train_target_data , decoder_val_target_data),\n",
        "    (val_input_lexicons , test_input_lexicons),\n",
        "    (val_target_lexicons , test_target_lexicons),\n",
        "    (num_encoder_tokens , num_decoder_tokens),\n",
        "    (max_encoder_seq_length , max_decoder_seq_length),\n",
        "    (target_token_index , inverse_input_token_index , inverse_target_token_index))"
      ],
      "metadata": {
        "id": "lDYTVYFU5_us"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the preprocessed data"
      ],
      "metadata": {
        "id": "ddNYxiS8ZSlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "((encoder_train_input_data , encoder_val_input_data , encoder_test_input_data),\n",
        " (decoder_train_input_data , decoder_val_input_data),\n",
        " (decoder_train_target_data , decoder_val_target_data),\n",
        " (val_input_lexicons , test_input_lexicons),\n",
        " (val_target_lexicons , test_target_lexicons),\n",
        " (num_encoder_tokens , num_decoder_tokens),\n",
        " (max_encoder_seq_length , max_decoder_seq_length),\n",
        " (target_token_index , inverse_input_token_index , inverse_target_token_index)) = preprocess_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qubNseHZWGm",
        "outputId": "0dbd518a-e1c9-4a14-c506-0a3f053069e2"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of the dataset :\n",
            "Number of train samples : 58550\n",
            "Number of val samples : 5683\n",
            "Number of test samples : 5747\n",
            "Number of unique input tokens : 27\n",
            "Number of unique output tokens : 66\n",
            "Max sequence length for inputs: 25\n",
            "Max sequence length for outputs: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nrG6C4YV5rga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8aLBHVDNdtwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}