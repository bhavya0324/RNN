# CS6910_Assignment_3

<strong> Dataset : </strong> dakshina_dataset_v1.0 </br>

<strong> Language Considered in the above Dataset : </strong> " Telugu Language " which is named as "te" folder in the dataset </br>

<strong> Upload the Following files in the google drive as follows : </strong> : </br>

1.) Download the dakshina_dataset_v1.0 from github and upload it in the google drive </br>

2.) Create a folder Named "Fonts" in google drive, then Download the nirmala.ttf file from github and upload it in the Fonts folder that you have created in the google drive </br>

<strong> This Repository has 3 ipynb files : </strong> <br/>

<strong> For No Attention part i.e., For Question : 1,2,3,4 </strong> : No_Attention.ipynb and No_Attention_cmd_line.ipynb <br/>
<br/>
<strong> For Attention part i.e., For Question : 5,6 </strong> : Attention.ipynb <br/>
<br/>
<br/>
<strong> Steps to Execute for Without Attention Mechanism part : </strong> <br/>
<br/>
<br/>
<strong> If you want to run No Attention part using Sweep Configuration then open No_Attention.ipynb file using the link to colab option and do the following </strong> : <br/>

1.) We have given 25 epochs if you want to change the number of epochs then go to sweep configuration and change epochs. <br/>

2.) There will be two blocks Actual sweep configuration (which we used to find best model) and Best model sweep configuration (Which we got as best configuration) <br/>

If you want to run for all sweeps then remove comments in actual sweep configuration block for sweep configuration and comment the best model sweep. <br/>

If you want to run the best sweep then run as it is without any changes.<br/>

If you want to add any parameters such as batch_size,beam_size, cell_type etc.., then go to sweep configuration and add or modify accordingly . <br/>

Now Run the cells from top to bottom <br/>

<strong> If you want to run No Attention part using Command line arguments then open No_Attention_cmd_line.ipynb and execute as follows : </strong> <br/>

1.)Input the Following values in run time , as following example input : <br/>

```
enter size of hidden layer : 256
enter number of encoder layers : 3
enter embedding size : 256
enter cell type : gru
enter dropout : 0.2
enter beam size : 0.0
enter recurrent dropout value : 0.1
enter learning algo, adam or rmsprop : adam
enter batch size : 256
enter number of epochs : 25
```
<br/>
After the Execution of any one of the ways above a file named <strong> predictions_vanilla_RNN.csv </strong> will be generated which contains the predictions made by the model on the Test data. The predictions file (predictions_vanilla_RNN.csv) generated by our model on test data is uploaded in predictions_vanilla folder in the github repo. </br>
<br/>
<strong> Steps to Execute for Attention part : </strong> <br/>
<br/>
<br/>
<strong> If you want to run Attention part using Sweep Configuration then open Attention.ipynb file using the link to colab option and do the following </strong> : <br/>

1.) We have given 25 epochs if you want to change the number of epochs then go to sweep configuration and change epochs. <br/>

2.) There will be two blocks Actual sweep configuration (which we used to find best model) and Best model sweep configuration (Which we got as best configuration) <br/>

If you want to run for all sweeps then remove comments in actual sweep configuration block for sweep configuration and comment the best model sweep. <br/>

If you want to run the best sweep then run as it is without any changes.<br/>

If you want to add any parameters such as batch_size,beam_size, cell_type etc.., then go to sweep configuration and add or modify accordingly . <br/>

Now Run the cells from top to bottom <br/>

<br/>
1.) After the Execution a file named <strong> predictions_attention_RNN.csv </strong> will be generated which contains the predictions made by the model on the Test data. The predictions file (predictions_attention_RNN.csv) generated by our best model on test data is uploaded in predictions_attention folder in the github repo </br>

2.) Attention heatmaps will be automatically generated and logged into wandb automatically </br>

3.) Connectivity visualization will be automatically generated and printed in the output. You can change the Number of samples to visualize in train() function by changing the variable <strong> samples </strong>. we have given 10 as number of samples and visualized the connectivity.</br>

4.) Lstm cell visualization will also be automatically generated and printed in the output. We have taken 10 samples and generated Lstm visualization for 3 random neurons. If you want to visualize for a particular neuron then write at the end of train function as follows : </br>

```
#neuron_no = random.choice(range(config.hidden_layer_size) #Taken randomly
neuron_no = particular_neuron_no 
print("Neuron :",neuron_no) 
visualize_lstm(samples, neuron_no) 
```
</br>
</br>

<strong> Steps to execute for Question 8 : </strong> </br>


1.)We have provided a folder named Question_8 in github . Open the folder and download train.txt which is taken from the link provided in wandb i.e., Kanye_West.txt from dataset-2 . Now create a folder named lyrics in google drive and upload train.txt in that folder </br>

2.)Now open the lyrics_generation.ipynb file in google colabs and run from top to bottom. </br>

Note : Please Run the above file in google colabs pro otherwise the memory will be exhausted and output error as Resource exhausted error. </br>

3.) After execution the lyrics will generated in the output. We committed the file with the outputs and pasted the lyrics in wandb </br>
